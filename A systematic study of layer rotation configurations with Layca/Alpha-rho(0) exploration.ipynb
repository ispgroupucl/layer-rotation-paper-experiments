{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook used for training of the five tasks of the paper (cfr. Table 1) with different alpha and rho(0) configurations (cfr. Section 3.1).\n",
    "\n",
    "Parameters that can be looped over to launch multiple experiments:\n",
    "- task solved\n",
    "- optimization method used (is always SGD_layca for this experiment)\n",
    "- alpha value\n",
    "- rho (0) value (i.e. the initial learning rate)\n",
    "\n",
    "The results (training curves, layer rotation curves and test  performance) are saved in a dictionary of depth 4.    \n",
    "A result can be easily found through: results[task][optimizer][alpha][lr]  \n",
    "The best results of a (task,optimizer) pair is saved in results[task][optimizer]\n",
    "\n",
    "The analysis of the results created by this notebook are done in the notebooks\n",
    "- Study of the relation between layer rotation and generalization\n",
    "- Study of the relation between layer rotation and training speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math as m\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from experiment_utils import history_todict, get_val_split\n",
    "from layer_rotation_utils import LayerRotationCurves\n",
    "\n",
    "from import_task import import_task\n",
    "from get_training_utils import get_training_schedule, get_stopping_criteria, get_optimizer, get_learning_rate_multipliers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities for storing the results in pickle files\n",
    "result_file = 'results.p'\n",
    "def load_results():\n",
    "    if not os.path.isfile(result_file):\n",
    "        return {}\n",
    "    else:\n",
    "        with open(result_file,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open(result_file,'wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, new_data):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    # new_data is a dictionary with the new (key,value) pairs\n",
    "    position.update(new_data)\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if results should be saved in the file or not\n",
    "save_results = True\n",
    "if not save_results:\n",
    "    results = {}\n",
    "# file for monitoring the experiment's progress\n",
    "monitor_file = 'monitor_experiment.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = ['C100-WRN']#['C10-CNN1','C100-resnet','tiny-CNN','C10-CNN2','C100-WRN']\n",
    "optimizers = ['SGD_layca']\n",
    "alphas = [-0.6]#[-0.8, -0.6, -0.4, -0.3, -0.2, -0.1, 0., 0.1, 0.2, 0.3, 0.4, 0.6, 0.8]\n",
    "lrs = [3**-3]#[3.**(-i) for i in range(-2,8)]\n",
    "\n",
    "for task in tasks:\n",
    "    x_train, y_train, x_test, y_test, get_model = import_task(task)\n",
    "    \n",
    "    # validation set is needed for early stopping or learning rate/alpha selection\n",
    "    [x_train, y_train], [x_val, y_val] = get_val_split(x_train,y_train, 0.1)\n",
    "    \n",
    "    # creates empty dictionary if first time the task is seen\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if task not in results.keys():\n",
    "            update_results([],{task:{}})\n",
    "    elif task not in results.keys():\n",
    "        results.update({task:{}})\n",
    "    \n",
    "    for optimizer in optimizers:        \n",
    "        if save_results:\n",
    "            results = load_results()\n",
    "            if optimizer not in results[task].keys():\n",
    "                update_results([task],{optimizer:{'history':{'history':{'val_acc':[-1]}}}}) # save a bad initial performance\n",
    "        elif optimizer not in results[task].keys():\n",
    "            results[task].update({optimizer:{'history':{'history':{'val_acc':[-1]}}}})\n",
    "     \n",
    "        for alpha in alphas:            \n",
    "            if save_results:\n",
    "                results = load_results()\n",
    "                if alpha not in results[task][optimizer].keys():\n",
    "                    update_results([task,optimizer],{alpha:{}})\n",
    "            elif alpha not in results[task][optimizer].keys():\n",
    "                results[task][optimizer].update({alpha:{}})\n",
    "            \n",
    "            for lr in lrs:\n",
    "                start = time.time()\n",
    "                model = get_model(weight_decay = 0.) if 'weight_decay' not in optimizer else get_model()\n",
    "\n",
    "                batch_size = 128\n",
    "                epochs, lr_scheduler = get_training_schedule(task,lr)\n",
    "                stop_callback = get_stopping_criteria(task)\n",
    "                verbose = 0\n",
    "                \n",
    "                # frequency at which cosine distance from initialization is computed\n",
    "                batch_frequency = int((x_train.shape[0]/batch_size))+5 # higher value than # of batches per epoch means once per epoch\n",
    "                ladc = LayerRotationCurves(batch_frequency = batch_frequency)\n",
    "\n",
    "                callbacks = [lr_scheduler, ladc] + stop_callback # get_stopping_criteria returns a list of callbacks\n",
    "\n",
    "                multipliers = get_learning_rate_multipliers(model,alpha = alpha)\n",
    "                metrics = ['accuracy', 'top_k_categorical_accuracy'] if 'tiny' in task else ['accuracy']\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer= get_optimizer(optimizer, lr, multipliers),\n",
    "                              metrics=metrics)\n",
    "\n",
    "                # cifar100 and tinyImagenet need early stopping\n",
    "                if task=='C100-resnet' or 'tiny' in task:\n",
    "                    weights_file = 'saved_weights/best_weights_'+str(np.random.randint(1e6))+'.h5'\n",
    "                    callbacks += [ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True, save_weights_only = True)]\n",
    "\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    if task in ['C10-CNN2','C100-WRN']:\n",
    "                        # data augmentation\n",
    "                        datagen = ImageDataGenerator(width_shift_range=0.125,\n",
    "                                 height_shift_range=0.125,\n",
    "                                 fill_mode='reflect',\n",
    "                                 horizontal_flip=True)\n",
    "\n",
    "                        warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                        history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                                      epochs = epochs,\n",
    "                                                      verbose = verbose,\n",
    "                                                      validation_data = (x_val, y_val),\n",
    "                                                      callbacks = callbacks)\n",
    "                    else:\n",
    "                        warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                        history = model.fit(x_train,y_train,\n",
    "                                            epochs = epochs,\n",
    "                                            batch_size = batch_size,\n",
    "                                            verbose = verbose,\n",
    "                                            validation_data = (x_val, y_val),\n",
    "                                            callbacks = callbacks)\n",
    "                \n",
    "                # application of early stopping\n",
    "                if task=='C100-resnet' or 'tiny' in task:\n",
    "                    model.load_weights(weights_file)\n",
    "\n",
    "                test_performance = model.evaluate(x_test,y_test, verbose = verbose)\n",
    "\n",
    "                if save_results:\n",
    "                    update_results([task, optimizer,alpha],{lr:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                                'test_performance':test_performance}})\n",
    "                else:\n",
    "                    results[task][optimizer][alpha].update({lr:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                                'test_performance':test_performance}})\n",
    "                \n",
    "                # if it beats current best validation performance of (task,optimizer) pair, update best performance\n",
    "                if save_results:\n",
    "                    results = load_results()\n",
    "                if max(history.history['val_acc']) > max(results[task][optimizer]['history']['history']['val_acc']):\n",
    "                    if save_results:\n",
    "                        update_results([task,optimizer],{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                         'test_performance':test_performance,'best_alpha':alpha,'best_lr':lr})\n",
    "                    else:\n",
    "                        results[task][optimizer].update({'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                         'test_performance':test_performance,'best_alpha':alpha,'best_lr':lr})\n",
    "                    \n",
    "                with open(monitor_file,'a') as file:\n",
    "                    file.write(task + ', '+optimizer+', '+str(alpha)+ ', '+str(lr)+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
